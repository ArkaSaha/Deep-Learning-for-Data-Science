{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "\n",
    "from random import randint\n",
    "import utils as ut \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import flags\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "#from util import get_normalized_data\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.model import CallableModelWrapper\n",
    "from cleverhans.utils import AccuracyReport\n",
    "from cleverhans.utils_pytorch import convert_pytorch_model_to_tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU with cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 (because we don't have anything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "from utils import check_cifar_dataset_exists\n",
    "data_path=check_cifar_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'cifar/train_data.pt')\n",
    "train_label=torch.load(data_path+'cifar/train_label.pt')\n",
    "test_data=torch.load(data_path+'cifar/test_data.pt')\n",
    "test_label=torch.load(data_path+'cifar/test_label.pt')\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "print(train_data.type())\n",
    "print(test_data.type())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4734)\n",
      "tensor(0.2516)\n"
     ]
    }
   ],
   "source": [
    "mean= train_data.mean()\n",
    "\n",
    "print(mean)\n",
    "\n",
    "std= train_data.std()\n",
    "\n",
    "print(std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG \n",
    "## (Running smooth AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(VGG_convnet, self).__init__()\n",
    "\n",
    "        # block 1:         3 x 32 x 32 --> 64 x 16 x 16        \n",
    "        self.conv1a = nn.Conv2d(3,   64,  kernel_size=3, padding=1 )\n",
    "        self.conv1b = nn.Conv2d(64,  64,  kernel_size=3, padding=1 )\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "        self.conv2a = nn.Conv2d(64,  128, kernel_size=3, padding=1 )\n",
    "        self.conv2b = nn.Conv2d(128, 128, kernel_size=3, padding=1 )\n",
    "        self.pool2  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # block 3:         128 x 8 x 8 --> 256 x 4 x 4        \n",
    "        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, padding=1 )\n",
    "        self.conv3b = nn.Conv2d(256, 256, kernel_size=3, padding=1 )\n",
    "        self.pool3  = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #block 4:          256 x 4 x 4 --> 512 x 2 x 2\n",
    "        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, padding=1 )\n",
    "        self.pool4  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        self.linear1 = nn.Linear(2048, 4096)\n",
    "        self.linear2 = nn.Linear(4096,4096)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear3 = nn.Linear(4096, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1:         3 x 32 x 32 --> 64 x 16 x 16\n",
    "        x = self.conv1a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # block 2:         64 x 16 x 16 --> 128 x 8 x 8\n",
    "        x = self.conv2a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # block 3:         128 x 8 x 8 --> 256 x 4 x 4\n",
    "        x = self.conv3a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        #block 4:          256 x 4 x 4 --> 512 x 2 x 2\n",
    "        x = self.conv4a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 10\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_convnet(\n",
      "  (conv1a): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2a): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3a): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4a): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "  (linear2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (linear3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n",
      "There are 27540554 (27.54 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net=VGG_convnet()\n",
    "\n",
    "print(net)\n",
    "ut.display_num_param(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "\n",
    "mean = mean.to(device)\n",
    "\n",
    "std = std.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.25 \n",
    "bs= 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the damn model (Please increase the number of epochs, I used just one to check if the code runs or not. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 \t time= 0.7035637656847636 min \t lr= 0.25 \t loss= 2.3031305736288084 \t error= 89.80458759895676 percent\n",
      "error rate on test set = 89.97231012658227 percent\n",
      " \n",
      "epoch= 2 \t time= 1.4526381254196168 min \t lr= 0.25 \t loss= 2.2992051983123547 \t error= 89.24952045730923 percent\n",
      "error rate on test set = 88.54825949367088 percent\n",
      " \n",
      "epoch= 3 \t time= 2.199037528038025 min \t lr= 0.25 \t loss= 2.302254942676905 \t error= 89.92846867312556 percent\n",
      "error rate on test set = 89.83386075949366 percent\n",
      " \n",
      "epoch= 4 \t time= 2.945137989521027 min \t lr= 0.25 \t loss= 2.2989800842216863 \t error= 89.15361252892048 percent\n",
      "error rate on test set = 88.71637658227847 percent\n",
      " \n",
      "epoch= 5 \t time= 3.6901373783747355 min \t lr= 0.25 \t loss= 2.280327234121845 \t error= 87.37571931556057 percent\n",
      "error rate on test set = 79.77650316455697 percent\n",
      " \n",
      "epoch= 6 \t time= 4.43919757604599 min \t lr= 0.25 \t loss= 2.186442126398501 \t error= 81.70756074168798 percent\n",
      "error rate on test set = 85.51226265822784 percent\n",
      " \n",
      "epoch= 7 \t time= 5.188654212156932 min \t lr= 0.25 \t loss= 1.9125894064183735 \t error= 69.80099104859335 percent\n",
      "error rate on test set = 62.32199367088608 percent\n",
      " \n",
      "epoch= 8 \t time= 5.939760422706604 min \t lr= 0.25 \t loss= 1.663446078824875 \t error= 60.654571614302036 percent\n",
      "error rate on test set = 54.65783227848101 percent\n",
      " \n",
      "epoch= 9 \t time= 6.689788818359375 min \t lr= 0.25 \t loss= 1.4326676767500466 \t error= 52.11117327060846 percent\n",
      "error rate on test set = 53.37223101265823 percent\n",
      " \n",
      "epoch= 10 \t time= 7.440283747514089 min \t lr= 0.125 \t loss= 1.1466418801975982 \t error= 41.23641304347826 percent\n",
      "error rate on test set = 40.249208860759495 percent\n",
      " \n",
      "epoch= 11 \t time= 8.192518758773804 min \t lr= 0.125 \t loss= 1.0251546310036994 \t error= 36.483375952981625 percent\n",
      "error rate on test set = 37.589003164556964 percent\n",
      " \n",
      "epoch= 12 \t time= 8.94350326458613 min \t lr= 0.125 \t loss= 0.8998131512680931 \t error= 31.763107407733305 percent\n",
      "error rate on test set = 36.501186708860764 percent\n",
      " \n",
      "epoch= 13 \t time= 9.695829923947652 min \t lr= 0.125 \t loss= 0.7812409483258377 \t error= 27.56393861282817 percent\n",
      "error rate on test set = 37.875791139240505 percent\n",
      " \n",
      "epoch= 14 \t time= 10.448300584157307 min \t lr= 0.125 \t loss= 0.6689434985218146 \t error= 23.603340792838875 percent\n",
      "error rate on test set = 32.021360759493675 percent\n",
      " \n",
      "epoch= 15 \t time= 11.201545858383179 min \t lr= 0.125 \t loss= 0.5569937445623491 \t error= 19.4489290189865 percent\n",
      "error rate on test set = 32.258702531645575 percent\n",
      " \n",
      "epoch= 16 \t time= 11.954886976877848 min \t lr= 0.125 \t loss= 0.43615811628758755 \t error= 15.309702679324333 percent\n",
      "error rate on test set = 33.41574367088608 percent\n",
      " \n",
      "epoch= 17 \t time= 12.7077388604482 min \t lr= 0.125 \t loss= 0.3367432280041068 \t error= 11.775095901830728 percent\n",
      "error rate on test set = 31.517009493670884 percent\n",
      " \n",
      "epoch= 18 \t time= 13.459453078111013 min \t lr= 0.125 \t loss= 0.25863902213628337 \t error= 8.993366368286445 percent\n",
      "error rate on test set = 31.517009493670884 percent\n",
      " \n",
      "epoch= 19 \t time= 14.212837119897207 min \t lr= 0.125 \t loss= 0.1814062124894708 \t error= 6.23960996527806 percent\n",
      "error rate on test set = 31.57634493670886 percent\n",
      " \n",
      "epoch= 20 \t time= 14.968065345287323 min \t lr= 0.0625 \t loss= 0.04152516145235323 \t error= 1.3467071611253196 percent\n",
      "error rate on test set = 29.33148734177215 percent\n",
      " \n",
      "epoch= 21 \t time= 15.721351850032807 min \t lr= 0.0625 \t loss= 0.007846260150951688 \t error= 0.16384271099744246 percent\n",
      "error rate on test set = 28.69857594936709 percent\n",
      " \n",
      "epoch= 22 \t time= 16.474337764581044 min \t lr= 0.0625 \t loss= 0.0025864863372824687 \t error= 0.06194053708439898 percent\n",
      "error rate on test set = 28.045886075949365 percent\n",
      " \n",
      "epoch= 23 \t time= 17.227786564826964 min \t lr= 0.0625 \t loss= 0.0009616002611800447 \t error= 0.009990409207161126 percent\n",
      "error rate on test set = 27.78876582278481 percent\n",
      " \n",
      "epoch= 24 \t time= 17.981672302881876 min \t lr= 0.0625 \t loss= 0.0024668594150591993 \t error= 0.0559462915601023 percent\n",
      "error rate on test set = 27.87776898734177 percent\n",
      " \n",
      "epoch= 25 \t time= 18.734956479072572 min \t lr= 0.0625 \t loss= 0.0017569200747617809 \t error= 0.033967391304347824 percent\n",
      "error rate on test set = 28.25356012658228 percent\n",
      " \n",
      "epoch= 26 \t time= 19.489233752091724 min \t lr= 0.0625 \t loss= 0.0007037590093323556 \t error= 0.0079923273657289 percent\n",
      "error rate on test set = 28.194224683544306 percent\n",
      " \n",
      "epoch= 27 \t time= 20.243191226323447 min \t lr= 0.0625 \t loss= 0.00030510336388577767 \t error= 0.0 percent\n",
      "error rate on test set = 27.966772151898734 percent\n",
      " \n",
      "epoch= 28 \t time= 20.996404365698496 min \t lr= 0.0625 \t loss= 0.00016144129168584615 \t error= 0.0 percent\n",
      "error rate on test set = 27.90743670886076 percent\n",
      " \n",
      "epoch= 29 \t time= 21.750004720687866 min \t lr= 0.0625 \t loss= 0.00010991105262157055 \t error= 0.0 percent\n",
      "error rate on test set = 27.99643987341772 percent\n",
      " \n",
      "epoch= 30 \t time= 22.503259742259978 min \t lr= 0.03125 \t loss= 8.168562369443753e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.867879746835445 percent\n",
      " \n",
      "epoch= 31 \t time= 23.25574870109558 min \t lr= 0.03125 \t loss= 8.239231222425885e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.946993670886076 percent\n",
      " \n",
      "epoch= 32 \t time= 24.008421727021535 min \t lr= 0.03125 \t loss= 7.540776069476115e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.97666139240506 percent\n",
      " \n",
      "epoch= 33 \t time= 24.763161261876423 min \t lr= 0.03125 \t loss= 7.253898989276417e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.79865506329114 percent\n",
      " \n",
      "epoch= 34 \t time= 25.51728630065918 min \t lr= 0.03125 \t loss= 6.449104422498543e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.90743670886076 percent\n",
      " \n",
      "epoch= 35 \t time= 26.272126432259878 min \t lr= 0.015625 \t loss= 5.5280030535919895e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.67998417721519 percent\n",
      " \n",
      "epoch= 36 \t time= 27.025959567228952 min \t lr= 0.015625 \t loss= 5.186751218098204e-05 \t error= 0.0 percent\n",
      "error rate on test set = 28.045886075949365 percent\n",
      " \n",
      "epoch= 37 \t time= 27.780389893054963 min \t lr= 0.015625 \t loss= 7.563984150539183e-05 \t error= 0.001998081841432225 percent\n",
      "error rate on test set = 27.90743670886076 percent\n",
      " \n",
      "epoch= 38 \t time= 28.532632489999134 min \t lr= 0.015625 \t loss= 5.521137085954642e-05 \t error= 0.0 percent\n",
      "error rate on test set = 28.02610759493671 percent\n",
      " \n",
      "epoch= 39 \t time= 29.285090720653535 min \t lr= 0.015625 \t loss= 5.3105106970811583e-05 \t error= 0.0 percent\n",
      "error rate on test set = 27.90743670886076 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,10000,bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs]\n",
    "        minibatch_label= test_label[i:i+bs]\n",
    "\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        #inputs = (minibatch_data - mean)/std\n",
    "        inputs = minibatch_data\n",
    "\n",
    "        scores=net( inputs ) \n",
    "\n",
    "        error = ut.get_error( scores , minibatch_label)\n",
    "\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')\n",
    "\n",
    "\n",
    "# ### Do 20 passes through the training set. Divide the learning rate by 2 at epoch 10, 14 and 18.\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "for epoch in range(1,40):\n",
    "    \n",
    "    # divide the learning rate by 2 at epoch 10, 14 and 18\n",
    "    if epoch==10 or epoch == 20 or epoch==30 or epoch==35:\n",
    "        my_lr = my_lr / 2\n",
    "    \n",
    "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    # set the order in which to visit the image from the training set\n",
    "    shuffled_indices=torch.randperm(50000)\n",
    " \n",
    "    for count in range(0,50000,bs):\n",
    "    \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices]\n",
    "        minibatch_label=  train_label[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # normalize the minibatch (this is the only difference compared to before!)\n",
    "        #inputs = (minibatch_data - mean)/std\n",
    "        inputs = minibatch_data\n",
    "        \n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores=net( inputs ) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass to compute dL/dU, dL/dV and dL/dW   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = ut.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1        \n",
    "    \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    \n",
    "\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min','\\t lr=', my_lr  ,'\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    eval_on_test_set() \n",
    "    print(' ')\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Pytorch and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_label))\n",
    "     \n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_label))\n",
    "\n",
    "sess = tf.Session()\n",
    "x_op1 = tf.placeholder(tf.float32, shape=(None,3, 32, 32,))\n",
    "#x_op2 = tf.placeholder(tf.float32, shape=(None,3, 32, 32,))\n",
    "\n",
    "\n",
    "# Convert pytorch model to a tf_model and wrap it in cleverhans\n",
    "tf_net = convert_pytorch_model_to_tf(net)\n",
    "cleverhans_model = CallableModelWrapper(tf_net, output_layer='logits')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attackkkkkk!! (Only one attack for now, I have more but won't use them right away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an FGSM attack\n",
    "fgsm_op = FastGradientMethod(cleverhans_model, sess=sess)\n",
    "fgsm_params = {'eps': 0.3,\n",
    "                 'clip_min': 0.,\n",
    "                 'clip_max': 1.}\n",
    "adv_x_op = fgsm_op.generate(x_op1, **fgsm_params)\n",
    "adv_preds_op = tf_net(adv_x_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model performance after the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_runs = 10000\n",
    "correct = 0\n",
    "\n",
    "dist_cifar_list = []\n",
    "for xs, ys in test_loader:\n",
    "    xs, ys = Variable(xs), Variable(ys)\n",
    "    adv_example = sess.run(adv_x_op, feed_dict={x_op1: xs})\n",
    "    adv_preds = sess.run(adv_preds_op, feed_dict={adv_x_op: adv_example})\n",
    "    dist_cifar_list.append( max(np.reshape(np.array(adv_example-xs), 3072)))\n",
    "    correct += (np.argmax(adv_preds, axis=1) == ys).sum()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial accuracy with FGSM attack: 17.900\n"
     ]
    }
   ],
   "source": [
    "dist_cifar = np.array(dist_cifar_list)  \n",
    "acc = float(correct) / no_runs\n",
    "\n",
    "print('Adversarial accuracy with FGSM attack: {:.3f}'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  46.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        9954.]),\n",
       " array([0.        , 0.03      , 0.06000001, 0.09000001, 0.12000002,\n",
       "        0.15000002, 0.18000002, 0.21000002, 0.24000004, 0.27000004,\n",
       "        0.30000004], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pyplot.hist(dist_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020300144\n"
     ]
    }
   ],
   "source": [
    "print(np.std(dist_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.attacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hesl\\ce7454_2018\\cleverhans\\cleverhans\\attacks.py:34: UserWarning: Argument back to attack constructors is not needed anymore and will be removed on or after 2019-03-26. All attacks are implemented using TensorFlow.\n",
      "  warnings.warn(\"Argument back to attack constructors is not needed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial accuracy with MomentumIterative method attack: 15.720\n"
     ]
    }
   ],
   "source": [
    "mi_op = MomentumIterativeMethod(cleverhans_model, sess=sess)\n",
    "\n",
    "mi_params = { 'eps' : 0.3, 'eps_iter' : 0.06, 'nb_iter' : 10} #'y': None, 'ord' : inf, 'decay_factor' : 1.0, 'clip_min' : None, 'clip_max' : None, 'y_target' : None}\n",
    "\n",
    "adv_x_op = mi_op.generate(x_op1, **mi_params)\n",
    "adv_preds_op = tf_net(adv_x_op)\n",
    "\n",
    "no_runs = 10000\n",
    "correct = 0\n",
    "for xs, ys in test_loader:\n",
    "    xs, ys = Variable(xs), Variable(ys)\n",
    "    adv_example = sess.run(adv_x_op, feed_dict={x_op1: xs})\n",
    "    adv_preds = sess.run(adv_preds_op, feed_dict={adv_x_op: adv_example})\n",
    "    correct += (np.argmax(adv_preds, axis=1) == ys).sum()\n",
    "\n",
    "acc = float(correct) / no_runs\n",
    "\n",
    "print('Adversarial accuracy with MomentumIterative method attack: {:.3f}'.format(acc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset missing - downloading...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from utils import check_mnist_dataset_exists\n",
    "data_path=check_mnist_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "test_data=torch.load(data_path+'mnist/test_data.pt')\n",
    "test_label=torch.load(data_path+'mnist/test_label.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,  output_size):\n",
    "        super(two_layer_net , self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size  , bias=False  )\n",
    "        self.layer2 = nn.Linear(  hidden_size  , output_size   , bias=False  )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        scores  = self.layer2(y_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_layer_net(\n",
      "  (layer1): Linear(in_features=784, out_features=50, bias=False)\n",
      "  (layer2): Linear(in_features=50, out_features=10, bias=False)\n",
      ")\n",
      "There are 39700 (0.04 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "net_basic=two_layer_net(784,50,10)\n",
    "\n",
    "print(net_basic)\n",
    "ut.display_num_param(net_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=torch.optim.Adam( net_basic.parameters() , lr=0.01 )\n",
    "\n",
    "bs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "net_basic = net_basic.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    for i in range(0,10000,bs):\n",
    "\n",
    "        minibatch_data =  test_data[i:i+bs].to(device)\n",
    "        minibatch_label= test_label[i:i+bs].to(device)\n",
    "\n",
    "        inputs = minibatch_data.view(bs,784)\n",
    "\n",
    "        scores=net_basic( inputs ) \n",
    "\n",
    "        error = ut.get_error( scores , minibatch_label)\n",
    "\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print( 'test error  = ', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch= 0 \t time= 7.598585605621338 \t loss= 0.2774459309654776 \t error= 8.239999185005823 percent\n",
      "test error  =  5.50999948978424 percent\n",
      " \n",
      "epoch= 5 \t time= 44.766631841659546 \t loss= 0.154446929949608 \t error= 3.8849998394648235 percent\n",
      "test error  =  4.559999573230743 percent\n",
      " \n",
      "epoch= 10 \t time= 83.4104700088501 \t loss= 0.13616184263938025 \t error= 3.3349999169508613 percent\n",
      "test error  =  5.0599995136260985 percent\n",
      " \n",
      "epoch= 15 \t time= 121.54804873466492 \t loss= 0.12343680095621001 \t error= 3.0283333003520965 percent\n",
      "test error  =  4.259999644756316 percent\n",
      " \n",
      "epoch= 20 \t time= 159.98262357711792 \t loss= 0.11049665675925015 \t error= 2.5500000536441805 percent\n",
      "test error  =  4.179999673366547 percent\n",
      " \n",
      "epoch= 25 \t time= 197.96348476409912 \t loss= 0.10943399854160159 \t error= 2.5316667298475903 percent\n",
      "test error  =  4.449999666213989 percent\n",
      " \n",
      "epoch= 30 \t time= 235.33126592636108 \t loss= 0.0977337130857065 \t error= 2.190000118811925 percent\n",
      "test error  =  4.309999716281891 percent\n",
      " \n",
      "epoch= 35 \t time= 273.67472410202026 \t loss= 0.10709828354520397 \t error= 2.261666772762934 percent\n",
      "test error  =  5.009999632835388 percent\n",
      " \n",
      "epoch= 40 \t time= 311.2481245994568 \t loss= 0.09966285729723412 \t error= 2.071666771173477 percent\n",
      "test error  =  4.119999718666077 percent\n",
      " \n",
      "epoch= 45 \t time= 347.3907399177551 \t loss= 0.0945565422086035 \t error= 1.9333334465821583 percent\n",
      "test error  =  4.519999599456788 percent\n",
      " \n",
      "epoch= 50 \t time= 384.3816521167755 \t loss= 0.09833980644793826 \t error= 1.923333446184794 percent\n",
      "test error  =  4.749999582767487 percent\n",
      " \n",
      "epoch= 55 \t time= 422.19679045677185 \t loss= 0.09195424660626134 \t error= 1.8433334648609163 percent\n",
      "test error  =  4.27999963760376 percent\n",
      " \n",
      "epoch= 60 \t time= 459.64777994155884 \t loss= 0.08546037420141483 \t error= 1.6933334489663443 percent\n",
      "test error  =  4.309999763965607 percent\n",
      " \n",
      "epoch= 65 \t time= 496.55326676368713 \t loss= 0.0916825958919582 \t error= 1.6266667902469636 percent\n",
      "test error  =  4.769999587535858 percent\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(70):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(60000)\n",
    " \n",
    "    for count in range(0,60000,bs):\n",
    "        \n",
    "        # forward and backward pass\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        indices=shuffled_indices[count:count+bs]\n",
    "        minibatch_data =  train_data[indices].to(device)\n",
    "        minibatch_label= train_label[indices].to(device)\n",
    "\n",
    "        inputs = minibatch_data.view(bs,784)\n",
    "\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        scores=net_basic( inputs ) \n",
    "\n",
    "        loss =  criterion( scores , minibatch_label) \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # compute some stats\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "               \n",
    "        error = ut.get_error( scores.detach() , minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches+=1\n",
    "    \n",
    "    \n",
    "    # once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches\n",
    "    \n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed_time = time.time() - start\n",
    "    \n",
    "    # every 10 epoch we display the stats \n",
    "    # and compute the error rate on the test set  \n",
    "    \n",
    "    if epoch % 5 == 0 : \n",
    "    \n",
    "        print(' ')\n",
    "        \n",
    "        print('epoch=',epoch, '\\t time=', elapsed_time,\n",
    "              '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "        \n",
    "        eval_on_test_set()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data.view(60000,784), train_label))\n",
    "     \n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data.view(10000,784), test_label))\n",
    "\n",
    "sess = tf.Session()\n",
    "x_op1 = tf.placeholder(tf.float32, shape=(None,784,))\n",
    "#x_op2 = tf.placeholder(tf.float32, shape=(None,3, 32, 32,))\n",
    "\n",
    "\n",
    "# Convert pytorch model to a tf_model and wrap it in cleverhans\n",
    "tf_net = convert_pytorch_model_to_tf(net_basic)\n",
    "cleverhans_model = CallableModelWrapper(tf_net, output_layer='logits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an FGSM attack\n",
    "fgsm_op = FastGradientMethod(cleverhans_model, sess=sess)\n",
    "fgsm_params = {'eps': 0.3,\n",
    "                 'clip_min': 0.,\n",
    "                 'clip_max': 1.}\n",
    "adv_x_op = fgsm_op.generate(x_op1, **fgsm_params)\n",
    "adv_preds_op = tf_net(adv_x_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_runs = 10000\n",
    "correct = 0\n",
    "\n",
    "dist_mnist_list = []\n",
    "for xs, ys in test_loader:\n",
    "    xs, ys = Variable(xs), Variable(ys)\n",
    "    adv_example = sess.run(adv_x_op, feed_dict={x_op1: xs})\n",
    "    adv_preds = sess.run(adv_preds_op, feed_dict={adv_x_op: adv_example})\n",
    "    dist_mnist_list.append( max(np.reshape(np.array(adv_example-xs), 784)))\n",
    "    correct += (np.argmax(adv_preds, axis=1) == ys).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial accuracy with FGSM attack for MNIST: 55.860\n"
     ]
    }
   ],
   "source": [
    "dist_mnist = np.array(dist_mnist_list)  \n",
    "acc = float(correct) / no_runs\n",
    "\n",
    "print('Adversarial accuracy with FGSM attack for MNIST: {:.3f}'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2869.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        7131.]),\n",
       " array([0.        , 0.03      , 0.06000001, 0.09000001, 0.12000002,\n",
       "        0.15000002, 0.18000002, 0.21000002, 0.24000004, 0.27000004,\n",
       "        0.30000004], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pyplot.hist(dist_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30000004\n"
     ]
    }
   ],
   "source": [
    "print(max (dist_mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial accuracy with MomentumIterative method attack on MNIST: 50.060\n"
     ]
    }
   ],
   "source": [
    "mi_op = MomentumIterativeMethod(cleverhans_model, sess=sess)\n",
    "\n",
    "mi_params = { 'eps' : 0.3, 'eps_iter' : 0.06, 'nb_iter' : 10} #'y': None, 'ord' : inf, 'decay_factor' : 1.0, 'clip_min' : None, 'clip_max' : None, 'y_target' : None}\n",
    "\n",
    "adv_x_op = mi_op.generate(x_op1, **mi_params)\n",
    "adv_preds_op = tf_net(adv_x_op)\n",
    "\n",
    "no_runs = 10000\n",
    "correct = 0\n",
    "for xs, ys in test_loader:\n",
    "    xs, ys = Variable(xs), Variable(ys)\n",
    "    adv_example = sess.run(adv_x_op, feed_dict={x_op1: xs})\n",
    "    adv_preds = sess.run(adv_preds_op, feed_dict={adv_x_op: adv_example})\n",
    "    correct += (np.argmax(adv_preds, axis=1) == ys).sum()\n",
    "\n",
    "acc = float(correct) / no_runs\n",
    "\n",
    "print('Adversarial accuracy with MomentumIterative method attack on MNIST: {:.3f}'.format(acc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
