{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "#device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR dataset missing - downloading...\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar/temp\\cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([10000, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_cifar_dataset_exists\n",
    "data_path=check_cifar_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'cifar/train_data.pt')\n",
    "train_label=torch.load(data_path+'cifar/train_label.pt')\n",
    "test_data=torch.load(data_path+'cifar/test_data.pt')\n",
    "test_label=torch.load(data_path+'cifar/test_label.pt')\n",
    "\n",
    "print(train_data.size())\n",
    "print(test_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4734)\n"
     ]
    }
   ],
   "source": [
    "mean= train_data.mean()\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2516)\n"
     ]
    }
   ],
   "source": [
    "std= train_data.std()\n",
    "\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Stem, self).__init__()\n",
    "        \n",
    "        #----------------------- stem block start ----------------------------\n",
    "        \n",
    "        #3 x 299 x 299 --> 32 x 149 x 149  , VALID Padding \n",
    "        self.conv1a = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0 ) \n",
    "        \n",
    "        #32 x 149 x 149 --> 32 x 147 x 147  , VALID Padding \n",
    "        self.conv1b = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0 )\n",
    "        \n",
    "        #32 x 147 x 147 --> 64 x 147 x 147  , SAME Padding \n",
    "        self.conv1c = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1 )\n",
    "        \n",
    "        #======================== Filter concat 1 =============================\n",
    "        \n",
    "        #kernel size = 3, VALID Padding\n",
    "        self.pool1  = nn.MaxPool2d(3, stride=2, padding=0 )\n",
    "        \n",
    "        #64 x 147 x 147 --> 96 x --- x ---  , VALID Padding   Find out the size of output\n",
    "        self.conv2a = nn.Conv2d(64, 96, kernel_size=3, stride=2, padding=0 )\n",
    "        \n",
    "        #======================== Filter concat 1 =============================\n",
    "        \n",
    "        #======================== Filter concat 2 =============================       \n",
    "        \n",
    "        #160 x 73 x 73 --> 64 x --- x ---  , SAME Padding   Find out the size of output\n",
    "        self.conv3a = nn.Conv2d(160, 64, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #64 x --- x --- --> 96 x --- x ---  , VALID Padding   Find out the size of output\n",
    "        self.conv3b = nn.Conv2d(64, 96, kernel_size=3, padding=0 )\n",
    "        \n",
    "        #160 x 73 x 73 --> 64 x --- x ---  , SAME Padding   Might be combined with self.conv1e1\n",
    "        self.conv4a = nn.Conv2d(160, 64, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #64 x --- x --- --> 64 x --- x ---  , SAME Padding   Find out the size of output\n",
    "        self.conv4b = nn.Conv2d(64, 64, kernel_size=[7,1], padding=1 )\n",
    "        \n",
    "        #64 x --- x --- --> 64 x --- x ---  , SAME Padding   Find out the size of output\n",
    "        self.conv4c = nn.Conv2d(64, 64, kernel_size=[1,7], padding=1 )\n",
    "        \n",
    "        #64 x --- x --- --> 96 x --- x ---  , VALID Padding   Might be combined with self.conv1e2\n",
    "        self.conv4d = nn.Conv2d(64, 96, kernel_size=3, padding=0 )\n",
    "        \n",
    "        #======================== Filter concat 2 ============================= \n",
    "        \n",
    "        #======================== Filter concat 3 =============================\n",
    "        \n",
    "        #192 x 71 x 71 --> 384 x 35 x 35  , VALID Padding \n",
    "        self.conv5a = nn.Conv2d(192, 384, kernel_size=3, padding=0 )\n",
    "         \n",
    "        #kernel size = 1, VALID Padding\n",
    "        self.pool2  = nn.MaxPool2d(1, stride=2, padding=0 )\n",
    "        \n",
    "        #======================== Filter concat 3 =============================\n",
    "        \n",
    "        #----------------------- stem block finish ----------------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1:         3 x 32 x 32 --> 64 x 16 x 16\n",
    "        x = self.conv1a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1c(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        xP = self.pool1(x)\n",
    "        xC = self.conv2a(x)\n",
    "        xC = F.relu(xC)\n",
    "        xFC1 = torch.cat((xP, xC), 1)\n",
    "        \n",
    "        y = self.conv3a(xFC1)\n",
    "        y = F.relu(y)\n",
    "        y = self.conv3b(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        z = self.conv4a(xFC1)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv4b(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv4c(xFC1)\n",
    "        z = F.relu(z)\n",
    "        z = self.conv4d(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        # Above code or this one?\n",
    "        #z = self.conv3a(xFC1)\n",
    "        #z = F.relu(z)\n",
    "        #z = self.conv4b(z)\n",
    "        #z = F.relu(z)\n",
    "        #z = self.conv4c(xFC1)\n",
    "        #z = F.relu(z)\n",
    "        #z = self.conv3b(z)\n",
    "        #z = F.relu(z)\n",
    "        \n",
    "        xFC2 = torch.cat((y, z), 1)\n",
    "        \n",
    "        xP = self.pool2(xFC2)\n",
    "        xC = self.conv5a(xFC2)\n",
    "        xC = F.relu(xC)\n",
    "        xFC3 = torch.cat((xP, xC), 1)   \n",
    "        \n",
    "        return xFC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        #----------------------- InceptionA block start ----------------------------\n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #block 1\n",
    "        #kernel size = 1, SAME Padding, OUTPUT = 384 Verify this\n",
    "        self.pool1 = nn.AvgPool2d(1, padding=1 )\n",
    "        \n",
    "        #384 x --- x --- --> 96 x --- x ---  , SAME Padding \n",
    "        self.conv1a = nn.Conv2d(384, 96, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 2\n",
    "        #384 x 35 x 35 --> 96 x --- x ---  , SAME Padding \n",
    "        self.conv2a = nn.Conv2d(384, 96, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 3\n",
    "        #384 x 35 x 35 --> 64 x --- x ---  , SAME Padding \n",
    "        self.conv3a = nn.Conv2d(384, 64, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #64 x --- x --- --> 96 x --- x ---  , SAME Padding \n",
    "        self.conv3b = nn.Conv2d(64, 96, kernel_size=3, padding=1 )\n",
    "        \n",
    "        #block 4\n",
    "        #384 x 35 x 35 --> 64 x --- x ---  , SAME Padding \n",
    "        self.conv4a = nn.Conv2d(384, 64, kernel_size=1, padding=1 )\n",
    "\n",
    "        #64 x --- x --- --> 96 x --- x ---  , SAME Padding \n",
    "        self.conv4b = nn.Conv2d(64, 96, kernel_size=3, padding=1 )\n",
    "        \n",
    "        #96 x --- x --- --> 96 x --- x ---  , SAME Padding \n",
    "        self.conv4c = nn.Conv2d(96, 96, kernel_size=3, padding=1 )\n",
    "        \n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #----------------------- InceptionA block finish ----------------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # block 1:\n",
    "        y = self.pool1(x)\n",
    "        y = self.conv1a(y)\n",
    "        y = F.relu(y)   # Do we need Relu here (after last operation)?\n",
    "        \n",
    "        # block 2:\n",
    "        z = self.conv2a(x)\n",
    "        z = F.relu(z) \n",
    "        \n",
    "        #block 3:\n",
    "        w = self.conv3a(x)\n",
    "        w = F.relu(w)\n",
    "        w = self.conv3b(w)\n",
    "        w = F.relu(w)\n",
    "        \n",
    "        #block 4:\n",
    "        v = self.conv4a(x)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4b(v)\n",
    "        v = F.relu(v)\n",
    "        \n",
    "        xFC = torch.cat((y, z, w, v), 1)\n",
    "        \n",
    "        return xFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(InceptionB, self).__init__()\n",
    "        \n",
    "        #----------------------- InceptionB block start ----------------------------\n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #block 1\n",
    "        #kernel size = 1, SAME Padding, OUTPUT = 384 Verify this\n",
    "        self.pool1 = nn.AvgPool2d(1, padding=1 )\n",
    "        \n",
    "        #1024 x --- x --- --> 128 x --- x ---  , SAME Padding \n",
    "        self.conv1a = nn.Conv2d(1024, 128, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 2\n",
    "        #1024 x 17 x 17 --> 384 x --- x ---  , SAME Padding \n",
    "        self.conv2a = nn.Conv2d(1024, 384, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 3\n",
    "        #1024 x 17 x 17 --> 192 x --- x ---  , SAME Padding \n",
    "        self.conv3a = nn.Conv2d(1024, 192, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #192 x --- x --- --> 224 x --- x ---  , SAME Padding \n",
    "        self.conv3b = nn.Conv2d(192, 224, kernel_size=[7,1], padding=1 )\n",
    "        \n",
    "        #224 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv3c = nn.Conv2d(224, 256, kernel_size=[1,7], padding=1 )\n",
    "        \n",
    "        #block 4\n",
    "        #1024 x 17 x 17 --> 192 x --- x ---  , SAME Padding \n",
    "        self.conv4a = nn.Conv2d(1024, 192, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #192 x --- x --- --> 192 x --- x ---  , SAME Padding \n",
    "        self.conv4b = nn.Conv2d(192, 192, kernel_size=[1,7], padding=1 )\n",
    "        \n",
    "        #192 x --- x --- --> 224 x --- x ---  , SAME Padding \n",
    "        self.conv4c = nn.Conv2d(192, 224, kernel_size=[7,1], padding=1 )\n",
    "        \n",
    "        #224 x --- x --- --> 224 x --- x ---  , SAME Padding \n",
    "        self.conv4d = nn.Conv2d(224, 224, kernel_size=[1,7], padding=1 )\n",
    "        \n",
    "        #224 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv4e = nn.Conv2d(224, 256, kernel_size=[7,1], padding=1 )\n",
    "        \n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #----------------------- InceptionB block finish ----------------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # block 1:\n",
    "        y = self.pool1(x)\n",
    "        y = self.conv1a(y)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        # block 2:\n",
    "        z = self.conv2a(x)\n",
    "        z = F.relu(z) \n",
    "        \n",
    "        #block 3:\n",
    "        w = self.conv3a(x)\n",
    "        w = F.relu(w)\n",
    "        w = self.conv3b(w)\n",
    "        w = F.relu(w)\n",
    "        w = self.conv3c(w)\n",
    "        w = F.relu(w)\n",
    "        \n",
    "        #block 4:\n",
    "        v = self.conv4a(x)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4b(v)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4c(v)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4d(v)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4e(v)\n",
    "        v = F.relu(v)\n",
    "        \n",
    "        xFC = torch.cat((y, z, w, v), 1)\n",
    "        \n",
    "        return xFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(InceptionC, self).__init__()\n",
    "        \n",
    "        #----------------------- InceptionC block start ----------------------------\n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #block 1\n",
    "        #kernel size = 1, SAME Padding, OUTPUT = 384 Verify this\n",
    "        self.pool1 = nn.AvgPool2d(1, padding=1 )\n",
    "        \n",
    "        #1536 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv1a = nn.Conv2d(1536, 256, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 2\n",
    "        #1536 x 8 x 8 --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv2a = nn.Conv2d(1536, 256, kernel_size=1, padding=1 ) \n",
    "        \n",
    "        #block 3\n",
    "        #1536 x 8 x 8 --> 384 x --- x ---  , SAME Padding \n",
    "        self.conv3a = nn.Conv2d(1536, 384, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #384 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv3b = nn.Conv2d(384, 256, kernel_size=[1,3], padding=1 )\n",
    "        \n",
    "        #384 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv3c = nn.Conv2d(384, 256, kernel_size=[3,1], padding=1 )\n",
    "        \n",
    "        #block 4\n",
    "        #1536 x 8 x 8 --> 384 x --- x ---  , SAME Padding \n",
    "        self.conv4a = nn.Conv2d(1536, 384, kernel_size=1, padding=1 )\n",
    "        \n",
    "        #384 x --- x --- --> 448 x --- x ---  , SAME Padding \n",
    "        self.conv4b = nn.Conv2d(384, 448, kernel_size=[1,3], padding=1 )\n",
    "        \n",
    "        #448 x --- x --- --> 512 x --- x ---  , SAME Padding \n",
    "        self.conv4c = nn.Conv2d(448, 512, kernel_size=[3,1], padding=1 )\n",
    "        \n",
    "        #512 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv4d = nn.Conv2d(512, 256, kernel_size=[3,1], padding=1 )\n",
    "        \n",
    "        #512 x --- x --- --> 256 x --- x ---  , SAME Padding \n",
    "        self.conv4e = nn.Conv2d(512, 256, kernel_size=[1,3], padding=1 )\n",
    "        \n",
    "        \n",
    "        #======================== Filter concat =============================\n",
    "        \n",
    "        #----------------------- InceptionC block finish ----------------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # block 1:\n",
    "        y = self.pool1(x)\n",
    "        y = self.conv1a(y)\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        # block 2:\n",
    "        z = self.conv2a(x)\n",
    "        z = F.relu(z) \n",
    "        \n",
    "        #block 3:\n",
    "        w = self.conv3a(x)\n",
    "        w = F.relu(w)\n",
    "        w1 = self.conv3b(w)\n",
    "        w1 = F.relu(w1)\n",
    "        w2 = self.conv3c(w)\n",
    "        w2 = F.relu(w2)\n",
    "        \n",
    "        #block 4:\n",
    "        v = self.conv4a(x)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4b(v)\n",
    "        v = F.relu(v)\n",
    "        v = self.conv4c(v)\n",
    "        v = F.relu(v)\n",
    "        v1 = self.conv4d(v)\n",
    "        v1 = F.relu(v1)\n",
    "        v2 = self.conv4e(v)\n",
    "        v2 = F.relu(v2)\n",
    "        \n",
    "        xFC = torch.cat((y, z, w1, w2, v1, v2), 1)\n",
    "        \n",
    "        return xFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_v4_convnet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Inception_v4_convnet, self).__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "       \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
